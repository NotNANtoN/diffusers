{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da506524-7e72-47ed-8d69-8993de74c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9ec50-ff6e-4ac4-bfb8-2d3994716246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from varying_aspect_ratio_dataset import create_df_from_parquets, assign_to_buckets\n",
    "\n",
    "max_files = 101\n",
    "\n",
    "cache = f\"laion_aesthetics_{max_files}.parquet\"\n",
    "\n",
    "#path = \"/hdd/data/finetune_SD/laion_aesthetics\"    \n",
    "path = \"../../../finetune_SD/laion_aesthetics_2/\" \n",
    "\n",
    "if os.path.exists(cache):\n",
    "    df = pd.read_parquet(cache)\n",
    "else:\n",
    "    #df = create_df_from_parquets(path, min_width=128, min_height=128, max_files=max_files)\n",
    "    #df = assign_to_buckets(df, bucket_step_size=64, max_width=768, max_height=768, min_bucket_count=64)\n",
    "    df = create_df_from_parquets(path, min_width=64, min_height=64, max_files=max_files)\n",
    "    df = assign_to_buckets(df, bucket_step_size=64, max_width=128, max_height=128, min_bucket_count=64)\n",
    "    df.to_parquet(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cf3f6-f27e-43c3-9bad-c30b0a5fc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.abs((df.width / df.height) - (df.bucket_width / df.bucket_height)).mean()\n",
    "# average ratio diff 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcfe36-7e8f-4eab-aca5-6b0c3fe1f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.width - df.bucket_width).mean() * (df.height - df.bucket_height).mean()\n",
    "# on average around 27 pixels lost when cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a87e48-8aea-4c81-9688-550e03420bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bff874-a2d5-4eb7-887c-ef570b758eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "#model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "model_path = \"../../../mus2vid/models/stable-diffusion-v1-5\"\n",
    "\n",
    "# Load models and create wrapper for stable diffusion\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39798168-3851-47bd-ada1-bf6382de3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from varying_aspect_ratio_dataset import BucketBatchSampler, BucketDataset, BucketSampler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "use_batch_sampler = True\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "if use_batch_sampler:\n",
    "    bucket_batch_sampler = BucketBatchSampler(df[\"bucket\"], batch_size=batch_size) \n",
    "    bucket_dataset = BucketDataset(df, tokenizer)\n",
    "\n",
    "    dataloader = DataLoader(bucket_dataset, batch_size=1, \n",
    "                            batch_sampler=bucket_batch_sampler, \n",
    "                            shuffle=False, \n",
    "                            num_workers=16, \n",
    "                            drop_last=False,\n",
    "                            pin_memory=True,\n",
    "                            )\n",
    "else:\n",
    "    bucket_batch_sampler = BucketSampler(df[\"bucket\"], batch_size=batch_size) \n",
    "    bucket_dataset = BucketDataset(df, tokenizer)\n",
    "\n",
    "    dataloader = DataLoader(bucket_dataset, batch_size=batch_size, \n",
    "                            sampler=bucket_batch_sampler, \n",
    "                            shuffle=False, \n",
    "                            num_workers=16, \n",
    "                            drop_last=False,\n",
    "                            pin_memory=True,\n",
    "                            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453d920-0e1e-4f6c-ad9a-3911385bde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%timeit bucket_dataset[random.randint(0, 1000)]\n",
    "# 5ms per img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf7e31-87bd-4c20-b55a-cbeb65633af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e29828-991b-441e-bef3-2f2868c076e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run(dl, steps, verbose=False):\n",
    "    iterator = iter(dl)\n",
    "    for i, batch in tqdm(enumerate(iterator), disable=not verbose):\n",
    "        img = batch[\"pixel_values\"].cuda(non_blocking=True)\n",
    "        if i == steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7faa-2dc1-4565-9e86-1434273f8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bucket_width.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e783b-0791-4de3-b7f2-8c67bc6045ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6f819-c13c-4d1a-886b-61bc876ebd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd334b77-cd5f-452d-8e59-241cea671f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2aab3d-bde8-4623-a0a2-df5607b295cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit run(dataloader, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d41ce-b6b8-416f-b0b2-84bcd829a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit run(dataloader, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c2e30-5158-43d0-8ade-400fd50ce906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with uncompressing (100 steps)\n",
    "# 1 worker - 3.32 s, 30.44 it/s\n",
    "# 2 worker - 2.34 s, 43.66 it/s\n",
    "# 4 worker - 1.8 s, 58.11 it/s\n",
    "# 8 worker - 1.3 s, 86.4 it/s\n",
    "# 16 worker - 1.4 s, 88.4 it/s\n",
    "\n",
    "\n",
    "# 1000 steps\n",
    "# 8 - 12.9 - 78.8 it/s\n",
    "# 16 - 10.5 - 93 it/s\n",
    "# 16 - 8.7 - 118 it/s\n",
    "\n",
    "\n",
    "# 1000 steps - img to cuda\n",
    "# 16 - 9.97, 8.5, 7.4, 7.3, 7.3 - mean=8.1\n",
    "\n",
    "# 1000 steps - img to cuda - pin memory\n",
    "# 16 - 8.2, 9.5, 8.0, 9.2, 7.8 - mean=8.5\n",
    "# .cuda(non_blocking=True) - 7.0, 7.4, 7.8, 7.0, 6,6, 6.6 = 6.8\n",
    "\n",
    "# best setting for batch size 1: 16 worker, pin_memory=True, .cuda(non_blocking=True)\n",
    "# with best setting using %timeit\n",
    "# bs 1: 4.23 s - 238 imgs/s == 4.2 ms\n",
    "# bs 2: 9.52 s\n",
    "# bs 4: 15.7 s\n",
    "# bs 8: 27.9 s - 286 images per second == 3.5 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb49385-7738-41bc-9aae-312aa1a38827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when loading from .tar files... :\n",
    "\n",
    "# 0 worker total: 0.14, 91.0, 4.5 s/it\n",
    "# 1 worker - 85.7 for its, 4.23 s/it\n",
    "# 4 worker - 354 for completion, 17.7 s/it\n",
    "\n",
    "\n",
    "# new sampler\n",
    "# 0 worker - 78.8, 3.9     # bs==2: 149, 7.45 it/s\n",
    "# 1 worker - 82.4, 4.1 s/it # bs2: 159, 7.9 it/s\n",
    "# 4 worker - 256 for completion, 12.8 s/it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1afc2ad-0040-4d0d-926e-c75f4d78c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(bucket_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75f8bc-fd75-408d-9ef5-08eb2344c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iterator).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a0e8d-bb59-4b55-89b6-04ef719722d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = bucket_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be09c74-574c-4784-b4e0-f33bf14172c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dced42-d9b1-450b-98a9-05dcadb2dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e8602-c91b-409e-8af6-35e9d4e2189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49ee05-42c9-4395-9600-3770b66102d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15580770-c2fc-46fd-be0e-3eff18fe9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d100478-6817-4a0f-891d-2a7c50c369c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = batch[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b7a77-89fe-44c4-887f-18e179ec33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "torchvision.transforms.ToPILImage()(imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595da2e-01b0-469d-aa67-3703ac85028f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
